{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Get texts and create lang-text dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dirs(directories):\n",
    "\n",
    "    for directory in directories:\n",
    "        if os.path.exists('./'+directory):\n",
    "            print(f\"The directory '{'./'+directory}' exists.\")\n",
    "        else:\n",
    "            os.makedirs(directory)\n",
    "            print(f\"The directory '{'./'+directory}' does not exist.\")\n",
    "\n",
    "def merge_sentences_pickle(sentences, pickle_file=None):\n",
    "    merged_text = ' '.join(sentences)\n",
    "    if pickle_file:\n",
    "        with open(pickle_file, 'wb') as file:\n",
    "            pickle.dump(merged_text, file)\n",
    "        print(f\"Merged text pickled to: {pickle_file}\")\n",
    "\n",
    "        file_size = os.path.getsize(pickle_file)\n",
    "        print(f\"Size of the pickled file: {file_size / (1024 * 1024)} MB with {len(sentences)} sentences!\")\n",
    "\n",
    "    return merged_text\n",
    "\n",
    "def download_treebank(url, output_path):\n",
    "    response = requests.get(url)\n",
    "    with open(output_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Downloaded treebank to {output_path}\")\n",
    "\n",
    "def extract_sentences(conllu_file, print_info=False):\n",
    "    sentences = []\n",
    "    with open(conllu_file, 'r', encoding='utf-8') as file:\n",
    "        sentence = []\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(' '.join(sentence))\n",
    "                    sentence = []\n",
    "            elif not line.startswith('#'):\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) > 1:\n",
    "                    sentence.append(parts[1])\n",
    "        if sentence:\n",
    "            sentences.append(' '.join(sentence))\n",
    "        \n",
    "    \n",
    "    if print_info == True:\n",
    "        print(f\"Number of sentences: {len(sentences)}\")\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory './texts/train/' exists.\n",
      "The directory './texts/test/' exists.\n",
      "The directory './texts_pickles/train/' exists.\n",
      "The directory './texts_pickles/test/' exists.\n"
     ]
    }
   ],
   "source": [
    "req_directories = [\"texts/train/\", \"texts/test/\", \"texts_pickles/train/\", \"texts_pickles/test/\"]\n",
    "check_dirs(req_directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded treebank to texts/train/en_ewt-ud-train.conllu\n",
      "Number of sentences: 12544\n",
      "Merged text pickled to: texts_pickles/train/en_ewt-ud-train.pkl\n",
      "Size of the pickled file: 1.00433349609375 MB with 12544 sentences!\n",
      "Sentence 1: Al - Zaman : American forces killed Shaikh Abdullah al - Ani , the preacher at the mosque in the town of Qaim , near the Syrian border .\n",
      "Sentence 2: [ This killing of a respected cleric will be causing us trouble for years to come . ]\n",
      "Sentence 3: DPA : Iraqi authorities announced that they had busted up 3 terrorist cells operating in Baghdad .\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/train/tr_kenet-ud-train.conllu\n",
      "Number of sentences: 15398\n",
      "Merged text pickled to: texts_pickles/train/tr_kenet-ud-train.pkl\n",
      "Size of the pickled file: 0.966766357421875 MB with 15398 sentences!\n",
      "Sentence 1: Üstünde lacivert abajurlu , parlak bir madenden lamba .\n",
      "Sentence 2: Bursa abanisi .\n",
      "Sentence 3: Efendi , sen de ne üstüme abanıyorsun ?\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/train/nl_alpino-ud-train.conllu\n",
      "Number of sentences: 12289\n",
      "Merged text pickled to: texts_pickles/train/nl_alpino-ud-train.pkl\n",
      "Size of the pickled file: 1.0122880935668945 MB with 12289 sentences!\n",
      "Sentence 1: Dat is alvast meegenomen , mocht over zes maanden blijken dat de inhoud tegenvalt .\n",
      "Sentence 2: De breuk in de federale regering van Joegoslavië kan volgens de Servische premier Djindjic nog worden gelijmd .\n",
      "Sentence 3: Met hervormingen , niet met nieuwe verkiezingen .\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/train/cs_fictree-ud-train.conllu\n",
      "Number of sentences: 10160\n",
      "Merged text pickled to: texts_pickles/train/cs_fictree-ud-train.pkl\n",
      "Size of the pickled file: 0.706995964050293 MB with 10160 sentences!\n",
      "Sentence 1: Měla na ruce nejkrásnější náramek , jaký jsem kdy viděl - secesní víla se na něm proplétala mezi brilianty a smaragdy .\n",
      "Sentence 2: Tenhle náramek měl cenu luxusního auta , jenže jeho krása byla ještě větší , byl to jeden z těch předmětů , na které člověk pohlédne a vzdává chválu Stvořiteli , který dal lidem takové nadání .\n",
      "Sentence 3: Stará dáma si všimla , že se na náramek dívám .\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/train/es_gsd-ud-train.conllu\n",
      "Number of sentences: 14187\n",
      "Merged text pickled to: texts_pickles/train/es_gsd-ud-train.pkl\n",
      "Size of the pickled file: 2.015753746032715 MB with 14187 sentences!\n",
      "Sentence 1: Además se le pediría a las empresas interesadas en prestar el servicio que se hagan cargo de la señalización y la cartelería que contiene información para los usuarios .\n",
      "Sentence 2: Producto del de el fin del de el imperio y las invasiones germanas la población isleña cayó a 1 millón durante el período sajón , permaneciendo hasta el siglo XI cuando volvió a aumentar llegando a 5 a 7 millones en el siglo XV , pero tras la peste negra volvió a reducirse reducir se a solo 2 a 4 millones .\n",
      "Sentence 3: MADRID , 3 ( EUROPA PRESS ) Las tenistas españolas Anabel Medina , Carla Suárez , María José Martínez , Nuria Llagostera , Arantxa Parra y Lourdes Domínguez han decidido retirar su plante para disputar la próxima eliminatoria de la Copa Federación tras llegar a un acuerdo con la Real Federación Española de Tenis ( RFET ) después de más de cuatro horas de reunión en el Consejo Superior de Deportes ( CSD ) con la mediación del de el secretario de Estado para el Deporte , Jaime Lissavetzky .\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/train/fi_tdt-ud-train.conllu\n",
      "Number of sentences: 12217\n",
      "Merged text pickled to: texts_pickles/train/fi_tdt-ud-train.pkl\n",
      "Size of the pickled file: 1.2152833938598633 MB with 12217 sentences!\n",
      "Sentence 1: Kävelyreitti III\n",
      "Sentence 2: Jäällä kävely avaa aina hauskoja ja erikoisia näkökulmia kaupunkiin .\n",
      "Sentence 3: Vähän samanlainen tunne kuin silloin , kun ystävämme vei meidät kerran ylös tuomiokirkon torniin .\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/train/sl_ssj-ud-train.conllu\n",
      "Number of sentences: 10903\n",
      "Merged text pickled to: texts_pickles/train/sl_ssj-ud-train.pkl\n",
      "Size of the pickled file: 1.1690044403076172 MB with 10903 sentences!\n",
      "Sentence 1: \" Tistega večera sem preveč popil , zgodilo se je mesec dni po tem , ko sem izvedel , da me žena vara .\n",
      "Sentence 2: Dogodek v Ankaranu je bila dramatična nesreča .\n",
      "Sentence 3: Dekle je ob vzvratni vožnji začelo vpiti , da bi jo utišal , sem prijel nož .\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/train/pl_lfg-ud-train.conllu\n",
      "Number of sentences: 13774\n",
      "Merged text pickled to: texts_pickles/train/pl_lfg-ud-train.pkl\n",
      "Size of the pickled file: 0.6011753082275391 MB with 13774 sentences!\n",
      "Sentence 1: 100-tysięcznym Grudziądzem rządzi lewica .\n",
      "Sentence 2: 102 lata obchodziła wczoraj Stanisława Przybyła , rodowita mieszkanka Łaz .\n",
      "Sentence 3: 11-latek przyniósł do szkoły w Studziankach granat .\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/train/de_gsd-ud-train.conllu\n",
      "Number of sentences: 13814\n",
      "Merged text pickled to: texts_pickles/train/de_gsd-ud-train.pkl\n",
      "Size of the pickled file: 1.600255012512207 MB with 13814 sentences!\n",
      "Sentence 1: Sehr gute Beratung , schnelle Behebung der Probleme , so stelle ich mir Kundenservice vor .\n",
      "Sentence 2: Die Kosten sind definitiv auch im in dem Rahmen .\n",
      "Sentence 3: Nette Gespräche , klasse Ergebnis\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/train/pt_gsd-ud-train.conllu\n",
      "Number of sentences: 9616\n",
      "Merged text pickled to: texts_pickles/train/pt_gsd-ud-train.pkl\n",
      "Size of the pickled file: 1.334451675415039 MB with 9616 sentences!\n",
      "Sentence 1: O objetivo dos de os principais hotéis da de a cidade é que o hóspede jamais tenha que sair dali e gaste a cada minuto da de a estadia .\n",
      "Sentence 2: Numa Em uma reunião entre representantes da de a Secretaria da de a Criança do de o DF e a juíza da de a Vara de Execuções de Medidas Socioeducativas , Lavínia Tupi Vieira Fonseca , ficou acordado que dos de os 25 internos , 12 serão internados na em a Unidade de Planaltina e os outros 13 devem retornar para a Unidade do de o Recanto das de as Emas , antigo Ciago .\n",
      "Sentence 3: Mas sem dúvida , com a educação de tempo integral , nós vamos colocar nossos jovens , as nossas crianças , às a as 7h da de a manhã na em a escola e eles vão sair no em o final da de a tarde , às a as 18h , devidamente alimentados , praticando esporte , tendo a profissionalização , e aí eles vão estar distante também da de a insegurança das de as ruas e mais longe ainda das de as drogas .\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "treebank_train_urls = [\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Turkish-Kenet/master/tr_kenet-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Dutch-Alpino/master/nl_alpino-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Czech-FicTree/master/cs_fictree-ud-train.conllu\",\n",
    "    # \"https://raw.githubusercontent.com/UniversalDependencies/UD_Swedish-Talbanken/master/sv_talbanken-ud-train.conllu\",\n",
    "    # \"https://raw.githubusercontent.com/UniversalDependencies/UD_Romanian-RRT/master/ro_rrt-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Spanish-GSD/master/es_gsd-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Finnish-TDT/master/fi_tdt-ud-train.conllu\",\n",
    "    # \"https://raw.githubusercontent.com/UniversalDependencies/UD_Croatian-SET/master/hr_set-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Slovenian-SSJ/master/sl_ssj-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Polish-LFG/master/pl_lfg-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/master/de_gsd-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Portuguese-GSD/master/pt_gsd-ud-train.conllu\"\n",
    "]\n",
    "\n",
    "\n",
    "treebank_train_info = {}\n",
    "\n",
    "for url_tree in treebank_train_urls:\n",
    "    file_name = url_tree.split('/')[-1]\n",
    "    output_file = \"texts/train/\"+  file_name\n",
    "    pickle_file = \"texts_pickles/train/\" + file_name.replace(\".conllu\", \".pkl\")\n",
    "\n",
    "    download_treebank(url_tree, output_file)\n",
    "    sentences = extract_sentences(output_file, print_info=True)\n",
    "    merge_sentences_pickle(sentences, pickle_file=pickle_file)\n",
    "\n",
    "    treebank_train_info[file_name.split(\".\")[0]] = len(sentences)\n",
    "    \n",
    "    for i, sentence in enumerate(sentences[:3]): \n",
    "        print(f\"Sentence {i+1}: {sentence}\")\n",
    "    print(\"-\"*20+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_ewt-ud-train': 12544,\n",
       " 'tr_kenet-ud-train': 15398,\n",
       " 'nl_alpino-ud-train': 12289,\n",
       " 'cs_fictree-ud-train': 10160,\n",
       " 'es_gsd-ud-train': 14187,\n",
       " 'fi_tdt-ud-train': 12217,\n",
       " 'sl_ssj-ud-train': 10903,\n",
       " 'pl_lfg-ud-train': 13774,\n",
       " 'de_gsd-ud-train': 13814,\n",
       " 'pt_gsd-ud-train': 9616}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_train_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded treebank to texts/test/en_ewt-ud-test.conllu\n",
      "Number of sentences: 2077\n",
      "<class 'list'>\n",
      "Sentences pickled to: texts_pickles/test/en_ewt-ud-test.pkl\n",
      "Sentence 1: What if Google Morphed Into GoogleOS ?\n",
      "Sentence 2: What if Google expanded on its search - engine ( and now e-mail ) wares into a full - fledged operating system ?\n",
      "Sentence 3: [ via Microsoft Watch from Mary Jo Foley ]\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/test/tr_kenet-ud-test.conllu\n",
      "Number of sentences: 1643\n",
      "<class 'list'>\n",
      "Sentences pickled to: texts_pickles/test/tr_kenet-ud-test.pkl\n",
      "Sentence 1: Cezveyi sürüyor , fincana boşaltıyor , kahveyi afiyetle içiyordu .\n",
      "Sentence 2: Kırmızı daha ağır basıyor .\n",
      "Sentence 3: Devlet adamlarının ileri gelenleri böyle sözlere karışmaz , ağır dururlar .\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/test/nl_alpino-ud-test.conllu\n",
      "Number of sentences: 596\n",
      "<class 'list'>\n",
      "Sentences pickled to: texts_pickles/test/nl_alpino-ud-test.pkl\n",
      "Sentence 1: GENUA - Kloosterorden en congregaties zullen in juli meedoen aan de protesten tegen de globalisering van de economie tijdens de top van de zeven grootste industrielanden en Rusland ( G8 ) in Genua .\n",
      "Sentence 2: Niet door luide demonstraties , maar door vasten en bidden willen de religieuzen hun solidariteit met de armen laten zien .\n",
      "Sentence 3: ,, Door te vasten laten we een typisch geweldloos geluid horen '' , zegt de Italiaanse franciscaan Alberto Tosini tegen Vidimus Dominum , webmagazine van religieuzen .\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/test/cs_fictree-ud-test.conllu\n",
      "Number of sentences: 1291\n",
      "<class 'list'>\n",
      "Sentences pickled to: texts_pickles/test/cs_fictree-ud-test.pkl\n",
      "Sentence 1: Když jsem si k babičce v neděli večer pro ni přišla , myslela jsem , že omdlím .\n",
      "Sentence 2: Udělala mi jiný límec !\n",
      "Sentence 3: Přece jsem jí podrobně vysvětlila , jaký má být výstřih u blůzy , jaký límeček , a ona mi udělá něco úplně jiného !\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/test/es_gsd-ud-test.conllu\n",
      "Number of sentences: 426\n",
      "<class 'list'>\n",
      "Sentences pickled to: texts_pickles/test/es_gsd-ud-test.pkl\n",
      "Sentence 1: De allí procedía la familia del de el escritor vallisoletano Blas Pajarero , cuya casa se encuentra en la Plaza de San Pedro ;\n",
      "Sentence 2: La Provincia de Mamoré es una provincia del de el departamento del de el Beni en Bolivia .\n",
      "Sentence 3: Con la llegada de maquinaria ( martillos hidráulicos y compresores ) , la perforación manual dejó de ser necesaria en las canteras y minas .\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/test/fi_tdt-ud-test.conllu\n",
      "Number of sentences: 1555\n",
      "<class 'list'>\n",
      "Sentences pickled to: texts_pickles/test/fi_tdt-ud-test.pkl\n",
      "Sentence 1: Taas teatteriin\n",
      "Sentence 2: Tänäänkin pitäisi mennä teatteriin .\n",
      "Sentence 3: Varasin pupulle ja minulle sekä sille sisarentyttärelleni , joka pääsi Turkuun lakia lukemaan , liput kaupunginteatterin Laulavat sadepisarat -musikaaliin .\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/test/sl_ssj-ud-test.conllu\n",
      "Number of sentences: 1282\n",
      "<class 'list'>\n",
      "Sentences pickled to: texts_pickles/test/sl_ssj-ud-test.pkl\n",
      "Sentence 1: Deloma se strinjam z drugim delom članka , ko opisuje finančne učinke reforme .\n",
      "Sentence 2: Namreč po zdravi \" kmečki pameti \" in lastnih izkušnjah še nobena reforma ni prinesla nečesa več , kvečjemu nekaj manj , predvsem pa na drugačen način , po možnosti boljši .\n",
      "Sentence 3: Reforme se ne delajo , ko imamo vsega dovolj , ampak , ko imamo nečesa premalo .\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/test/pl_lfg-ud-test.conllu\n",
      "Number of sentences: 1727\n",
      "<class 'list'>\n",
      "Sentences pickled to: texts_pickles/test/pl_lfg-ud-test.pkl\n",
      "Sentence 1: 569 lat temu spalono na stosie Joannę D'Arc .\n",
      "Sentence 2: 6 przestępców uciekło ze szpitala psychiatrycznego w Radomiu .\n",
      "Sentence 3: A co by było , gdyby były dwa albo trzy ?\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/test/de_gsd-ud-test.conllu\n",
      "Number of sentences: 977\n",
      "<class 'list'>\n",
      "Sentences pickled to: texts_pickles/test/de_gsd-ud-test.pkl\n",
      "Sentence 1: Der Hauptgang war in Ordnung , aber alles andere als umwerfend .\n",
      "Sentence 2: Ich habe dort 2007 meinen OWD gemacht und weil mir das Tauchen so gefiel hab ich dort noch im in dem selben Jahr den AOWD und den Deep drangehängt .\n",
      "Sentence 3: Ist ja wohl ein Witz .\n",
      "--------------------\n",
      "\n",
      "Downloaded treebank to texts/test/pt_gsd-ud-test.conllu\n",
      "Number of sentences: 1200\n",
      "<class 'list'>\n",
      "Sentences pickled to: texts_pickles/test/pt_gsd-ud-test.pkl\n",
      "Sentence 1: Mas por não existir um marco legal há uma insegurança por parte dos de os investidores \" , destacou .\n",
      "Sentence 2: O executivo explicou que a companhia vai manter o foco na em a produtividade das de as consultoras e na em a evolução do de o nível de serviços .\n",
      "Sentence 3: Com o resultado Cairoli retorna á liderança da de a competição .\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "treebank_test_urls = [\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Turkish-Kenet/master/tr_kenet-ud-test.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Dutch-Alpino/master/nl_alpino-ud-test.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Czech-FicTree/master/cs_fictree-ud-test.conllu\",\n",
    "    # \"https://raw.githubusercontent.com/UniversalDependencies/UD_Swedish-Talbanken/master/sv_talbanken-ud-test.conllu\",\n",
    "    # \"https://raw.githubusercontent.com/UniversalDependencies/UD_Romanian-RRT/master/ro_rrt-ud-test.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Spanish-GSD/master/es_gsd-ud-test.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Finnish-TDT/master/fi_tdt-ud-test.conllu\",\n",
    "    # \"https://raw.githubusercontent.com/UniversalDependencies/UD_Croatian-SET/master/hr_set-ud-test.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Slovenian-SSJ/master/sl_ssj-ud-test.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Polish-LFG/master/pl_lfg-ud-test.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/master/de_gsd-ud-test.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Portuguese-GSD/master/pt_gsd-ud-test.conllu\"\n",
    "]\n",
    "\n",
    "\n",
    "treebank_test_info = {}\n",
    "\n",
    "for url_tree in treebank_test_urls:\n",
    "    file_name = url_tree.split('/')[-1]\n",
    "    output_file = \"texts/test/\"+  file_name\n",
    "    pickle_file = \"texts_pickles/test/\" + file_name.replace(\".conllu\", \".pkl\")\n",
    "\n",
    "    download_treebank(url_tree, output_file)\n",
    "\n",
    "    sentences = extract_sentences(output_file, print_info=True)\n",
    "    print(type(sentences))\n",
    "    with open(pickle_file, 'wb') as file:\n",
    "        pickle.dump(sentences, file)\n",
    "        print(f\"Sentences pickled to: {pickle_file}\")\n",
    "\n",
    "\n",
    "    treebank_test_info[file_name.split(\".\")[0]] = len(sentences)\n",
    "    \n",
    "    for i, sentence in enumerate(sentences[:3]): \n",
    "        print(f\"Sentence {i+1}: {sentence}\")\n",
    "    print(\"-\"*20+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_ewt-ud-test': 2077,\n",
       " 'tr_kenet-ud-test': 1643,\n",
       " 'nl_alpino-ud-test': 596,\n",
       " 'cs_fictree-ud-test': 1291,\n",
       " 'es_gsd-ud-test': 426,\n",
       " 'fi_tdt-ud-test': 1555,\n",
       " 'sl_ssj-ud-test': 1282,\n",
       " 'pl_lfg-ud-test': 1727,\n",
       " 'de_gsd-ud-test': 977,\n",
       " 'pt_gsd-ud-test': 1200}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_test_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Load pickled text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs', 'de', 'en', 'es', 'fi', 'nl', 'pl', 'pt', 'sl', 'tr']\n"
     ]
    }
   ],
   "source": [
    "dict_texts_train = {}\n",
    "\n",
    "for pickle_name in os.listdir(\"texts_pickles/train/\"):\n",
    "\n",
    "    lang = pickle_name.split(\"_\")[0]\n",
    "    pickle_path = \"texts_pickles/train/\" + pickle_name\n",
    "\n",
    "    with open(pickle_path, 'rb') as file:\n",
    "        dict_texts_train[lang] = pickle.load(file)\n",
    "\n",
    "print(list(dict_texts_train.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Además se le pediría a las empresas interesadas en prestar el servicio que se hagan cargo de la señalización y la cartelería que contiene información para los usuarios . Producto del de el fin del de el imperio y las invasiones germanas la población isleña cayó a 1 millón durante el período sajón , permaneciendo hasta el siglo XI cuando volvió a aumentar llegando a 5 a 7 millones en el siglo XV , pero tras la peste negra volvió a reducirse reducir se a solo 2 a 4 millones . MADRID , 3 ( EUROPA P'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_texts_train[\"es\"][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs', 'de', 'en', 'es', 'fi', 'nl', 'pl', 'pt', 'sl', 'tr']\n"
     ]
    }
   ],
   "source": [
    "dict_texts_test = {}\n",
    "\n",
    "for pickle_name in os.listdir(\"texts_pickles/test/\"):\n",
    "\n",
    "    lang = pickle_name.split(\"_\")[0]\n",
    "    pickle_path = \"texts_pickles/test/\" + pickle_name\n",
    "\n",
    "    with open(pickle_path, 'rb') as file:\n",
    "        dict_texts_test[lang] = pickle.load(file)\n",
    "\n",
    "print(list(dict_texts_test.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['De allí procedía la familia del de el escritor vallisoletano Blas Pajarero , cuya casa se encuentra en la Plaza de San Pedro ;',\n",
       " 'La Provincia de Mamoré es una provincia del de el departamento del de el Beni en Bolivia .',\n",
       " 'Con la llegada de maquinaria ( martillos hidráulicos y compresores ) , la perforación manual dejó de ser necesaria en las canteras y minas .',\n",
       " 'Su actuación recibió buenas reseñas , al a el igual que el filme .',\n",
       " 'Aunque se solía sobreentender que Van Vliet fue aprendiz de Rembrandt , en realidad era un colaborador ya formado , que produjo planchas copiando pinturas de Rembrandt y que llegó a intervenir a dúo con él en otras .']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_texts_test[\"es\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Language Detection and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood method:\n",
      "The detected language is: tr\n",
      "{'cs': -473.11114387715264, 'de': -334.46466670644753, 'en': -379.0041326716339, 'es': -374.21843353361203, 'fi': -386.7071045916229, 'nl': -390.650450483091, 'pl': -422.35818138293956, 'pt': -389.76711262008234, 'sl': -375.8340633959712, 'tr': -322.75569937102216}\n",
      "----------------------------------------------------------------------\n",
      "Distance method:\n",
      "The detected language is: de\n",
      "{'cs': 0.026841066011374332, 'de': 0.021062889030402578, 'en': 0.023506366280480445, 'es': 0.023588717243529875, 'fi': 0.022267368299029547, 'nl': 0.021439784559633652, 'pl': 0.027299296131773485, 'pt': 0.024921214667465007, 'sl': 0.025563777141148623, 'tr': 0.022434087389427673}\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text, n):\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "def create_token_freq_table(text, n):\n",
    "\n",
    "    # 1) create tokens for given text\n",
    "    tokens = tokenize(text, n)\n",
    "    # print(tokens[:500])\n",
    "\n",
    "    # 2) create character frequency table\n",
    "    token_freqs = {char: 0 for char in set(tokens)}\n",
    "    # print(len(token_freqs))\n",
    "    # print(token_freqs)\n",
    "\n",
    "\n",
    "    total_tokens = 0\n",
    "    # 3) set character frequencies and normalize them\n",
    "    for token in tokens:\n",
    "        if token in token_freqs:\n",
    "            token_freqs[token] += 1\n",
    "            total_tokens += 1\n",
    "\n",
    "    for token in token_freqs:\n",
    "        token_freqs[token] /= total_tokens  # normalize frequencies\n",
    "    \n",
    "    return token_freqs\n",
    "\n",
    "def detect_lang_distances(text, lang_token_freqs, n):\n",
    "    lang_dists = {}\n",
    "    for lang, lang_token_freq in lang_token_freqs.items():\n",
    "        # 1) create tokens and token-freq table for given text\n",
    "        tokens = tokenize(text, n)\n",
    "        text_token_freq = create_token_freq_table(text, n)\n",
    "\n",
    "        # 2) compare distances of tokens of the text with other languages' tokens\n",
    "        distance = 0.0\n",
    "        for tok in tokens:\n",
    "            text_token = text_token_freq.get(tok, 0)\n",
    "            lang_token = lang_token_freq.get(tok, 0)\n",
    "            distance +=  (text_token - lang_token) ** 2\n",
    "\n",
    "        lang_dists[lang] = distance\n",
    "\n",
    "    closest_lang = min(lang_dists, key=lang_dists.get)  \n",
    "    return closest_lang, lang_dists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_lang_likelihood(text, lang_token_freqs, n, penalizer_val=1e-5):\n",
    "    lang_scores = {}\n",
    "    for lang, lang_token_freq in lang_token_freqs.items():\n",
    "\n",
    "        # 1) create tokens for given text\n",
    "        tokens = tokenize(text, n)\n",
    "\n",
    "        # 2) calculate likelihood of languages with the given text\n",
    "        lang_score = 0\n",
    "        for token in tokens:\n",
    "            if token in lang_token_freq:\n",
    "                lang_score += math.log(lang_token_freq[token])\n",
    "            else:\n",
    "                lang_score += math.log(penalizer_val)  \n",
    "\n",
    "        lang_scores[lang] = lang_score\n",
    "        \n",
    "    closest_lang = max(lang_scores, key=lang_scores.get)\n",
    "    return closest_lang, lang_scores\n",
    "\n",
    "def main():\n",
    "    N_GRAM_VAL = 2\n",
    "\n",
    "    # 1) Create language representations = token frequency tables\n",
    "    lang_token_freqs = {}\n",
    "    for lang, text in dict_texts_train.items():\n",
    "        lang_token_freqs[lang] = create_token_freq_table(text,\n",
    "                                                         n=N_GRAM_VAL)\n",
    "    \n",
    "    # 2) Set your text\n",
    "    YOUR_TEXT = \"Merhaba, ben Unat! Bilgisayar mühendisiyim!\"\n",
    "\n",
    "\n",
    "    # 3) Detection methods\n",
    "    # 3.1) Likelihood Method\n",
    "\n",
    "    detected_lang, likelihoods = detect_lang_likelihood(YOUR_TEXT, \n",
    "                                                        lang_token_freqs, \n",
    "                                                        n=N_GRAM_VAL,\n",
    "                                                        penalizer_val=1e-12)\n",
    "\n",
    "    print(\"Likelihood method:\")    \n",
    "    print(f'The detected language is: {detected_lang}')\n",
    "    print(likelihoods)\n",
    "\n",
    "    print(\"-\"*70)\n",
    "\n",
    "\n",
    "    # 3.2) Distance Method\n",
    "\n",
    "    detected_lang, distances = detect_lang_distances(YOUR_TEXT, \n",
    "                                                     lang_token_freqs, \n",
    "                                                     n=N_GRAM_VAL)\n",
    "\n",
    "    print(\"Distance method:\")    \n",
    "    print(f'The detected language is: {detected_lang}')\n",
    "    print(distances)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_conf_matrix(languages):\n",
    "    size = len(languages)\n",
    "    return [[0] * size for _ in range(size)]\n",
    "\n",
    "\n",
    "def print_conf_matrix(matrix, languages):\n",
    "    print(\"Horizontally: Predicted Languages, Vertically: True Languages\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"  \"+\"\\t\".join(languages))\n",
    "    for idx, row in enumerate(matrix):\n",
    "        print(languages[idx], \"\\t\".join(map(str, row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood method confusion matrix (N=3):\n",
      "Horizontally: Predicted Languages, Vertically: True Languages\n",
      "Confusion Matrix:\n",
      "  cs\tde\ten\tes\tfi\tnl\tpl\tpt\tsl\ttr\n",
      "cs 1267\t1\t1\t1\t2\t1\t3\t4\t7\t4\n",
      "de 1\t949\t2\t0\t2\t3\t2\t3\t11\t4\n",
      "en 39\t48\t1868\t18\t19\t19\t12\t20\t12\t22\n",
      "es 0\t0\t1\t418\t0\t0\t0\t2\t0\t5\n",
      "fi 1\t4\t6\t1\t1527\t2\t0\t0\t2\t12\n",
      "nl 0\t1\t2\t5\t0\t586\t1\t0\t0\t1\n",
      "pl 1\t0\t1\t0\t2\t0\t1720\t0\t3\t0\n",
      "pt 0\t0\t5\t7\t1\t0\t0\t1186\t0\t1\n",
      "sl 5\t1\t1\t1\t0\t0\t2\t3\t1269\t0\n",
      "tr 0\t0\t0\t0\t0\t0\t0\t0\t0\t1643\n",
      "\n",
      "Distance method confusion matrix (N=3):\n",
      "Horizontally: Predicted Languages, Vertically: True Languages\n",
      "Confusion Matrix:\n",
      "  cs\tde\ten\tes\tfi\tnl\tpl\tpt\tsl\ttr\n",
      "cs 1108\t4\t10\t12\t4\t19\t32\t35\t65\t2\n",
      "de 3\t837\t4\t1\t3\t109\t5\t2\t11\t2\n",
      "en 85\t66\t1673\t41\t32\t59\t22\t64\t19\t16\n",
      "es 2\t0\t2\t394\t0\t0\t0\t25\t3\t0\n",
      "fi 9\t6\t6\t3\t1418\t89\t3\t3\t9\t9\n",
      "nl 3\t10\t3\t4\t1\t571\t1\t2\t1\t0\n",
      "pl 19\t9\t14\t8\t15\t10\t1556\t39\t55\t2\n",
      "pt 4\t2\t3\t14\t0\t1\t0\t1174\t2\t0\n",
      "sl 18\t1\t2\t3\t3\t7\t1\t7\t1238\t2\n",
      "tr 12\t15\t0\t21\t5\t66\t0\t12\t6\t1506\n"
     ]
    }
   ],
   "source": [
    "N_GRAM_VAL = 3\n",
    "\n",
    "# 1) Create language representations = token frequency tables\n",
    "lang_token_freqs = {}\n",
    "for lang, text in dict_texts_train.items():\n",
    "    lang_token_freqs[lang] = create_token_freq_table(text,\n",
    "                                                     n=N_GRAM_VAL)\n",
    "\n",
    "# 2) Get languages and corresponding indexes\n",
    "languages = list(dict_texts_train.keys())\n",
    "lang_index = {lang: idx for idx, lang in enumerate(languages)}\n",
    "\n",
    "# 3) Initialize conf matrix\n",
    "confusion_matrix_likelihood = initialize_conf_matrix(languages)\n",
    "confusion_matrix_distance = initialize_conf_matrix(languages)\n",
    "\n",
    "def update_conf_matrix(matrix, lang_index, true_lang, predicted_lang):\n",
    "    true_idx = lang_index[true_lang]\n",
    "    pred_idx = lang_index[predicted_lang]\n",
    "    matrix[true_idx][pred_idx] += 1\n",
    "\n",
    "# 4) Predict sentences for each language \n",
    "for true_lang, sentences in dict_texts_test.items():\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        t1_likelihood = time.time()\n",
    "        pred_lang_likelihood, _ = detect_lang_likelihood(sentence, lang_token_freqs, n=N_GRAM_VAL)\n",
    "        true_idx = lang_index[true_lang]\n",
    "        pred_idx = lang_index[pred_lang_likelihood]\n",
    "        confusion_matrix_likelihood[true_idx][pred_idx] += 1\n",
    "        t2_likelihood = time.time()\n",
    "\n",
    "\n",
    "        t1_distances = time.time()\n",
    "        pred_lang_distance, _ = detect_lang_distances(sentence, lang_token_freqs, n=N_GRAM_VAL)\n",
    "        # true_idx = lang_index[true_lang]\n",
    "        pred_idx = lang_index[pred_lang_distance]\n",
    "        confusion_matrix_distance[true_idx][pred_idx] += 1\n",
    "        t2_distances = time.time()\n",
    "\n",
    "print(f\"Likelihood method confusion matrix (N={N_GRAM_VAL}):\")\n",
    "print_conf_matrix(confusion_matrix_likelihood, languages)\n",
    "# print(f\"Elapsed time: {t2_likelihood-t1_likelihood}\")\n",
    "\n",
    "print(f\"\\nDistance method confusion matrix (N={N_GRAM_VAL}):\")\n",
    "print_conf_matrix(confusion_matrix_distance, languages)\n",
    "# print(f\"Elapsed time: {t2_distances-t1_distances}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
