{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Data"
      ],
      "metadata": {
        "id": "RUZwuJbxbJo5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FJEghUjKbBgB"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtX0wW98bMkl",
        "outputId": "c0f2003b-aa05-472b-83fb-3ea9b387118c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESjpXdWnbPLc",
        "outputId": "70f6dd0f-a7b4-4978-ec8a-d622eaddd30b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))   # unique chars\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlF5rpCqbQ9k",
        "outputId": "8a61f7e0-590a-48fe-d31f-7b163a786cdf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize: convert raw text as string to some\n",
        "# sequence of integers.\n",
        "\n",
        "\n",
        "# lookup table: mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n",
        "\n",
        "\n",
        "## google uses sentencepiece(sub-word units),\n",
        "## openai uses tiktoken\n",
        "\n",
        "## trade-off: code book size - sequence lengths:\n",
        "## long seq of integers <=> small vocabs\n",
        "## short seq of integers <=> large vocabs\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYNF7g_cbbpZ",
        "outputId": "7e8dd435-9a65-4494-f80e-1cbd29597f75"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stoi)\n",
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFfl6sQGcEB0",
        "outputId": "d9677546-109e-4e11-f703-34ac326a8eda"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
            "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "encoded_texts = encode(text)\n",
        "data = torch.tensor(encoded_texts, dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVWYrSJ4eDkv",
        "outputId": "a6783ba2-5d60-4aa7-b353-58e9a19daf6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "print(f\"{n} + {int(0.1*len(data))} = {len(data)}\")\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je3IckhIecLo",
        "outputId": "9dd9279d-343e-4f55-c721-c574327fadd5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1003854 + 111539 = 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train just chunks (max length)\n",
        "block_size = 8\n",
        "train_data[:block_size+1]\n",
        "\n",
        "### simultaneously predict every one of these characters\n",
        "###"
      ],
      "metadata": {
        "id": "YkXYiLZQe2wv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab22301f-186e-4c73-96af-e994a6eacc54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]     # input to transformer\n",
        "y = train_data[1:block_size+1]  # next block size characters\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")\n",
        "\n",
        "\n",
        "# context 1\n",
        "# ...\n",
        "# context 8 (block size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETBtWRw1HdxO",
        "outputId": "47e71cb1-79ba-4855-b076-5dd447168f6c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "\n",
        "    # random positions to get chunk == random offset into the training set\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    # torch stack ==> makes them row by row\n",
        "    # one dim tensor ==> 4x8 tensor\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('(X)inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('(Y)targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension (row)\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C6i7DESIrX5",
        "outputId": "09b3438e-0bc8-4ae7-b164-c33ca881676d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(X)inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "(Y)targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer\n",
        "\n",
        "## simplest NN model in language modeling = bigram lang model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5zBijGoJslI",
        "outputId": "e778526a-675f-4b60-ed71-d2c4e1421c20"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # import torch\n",
        "# # import torch.nn as nn\n",
        "# # from torch.nn import functional as F\n",
        "# # torch.manual_seed(1337)\n",
        "\n",
        "# # class BigramLanguageModel(nn.Module):\n",
        "\n",
        "# #     def __init__(self, vocab_size):\n",
        "# #         super().__init__()\n",
        "# #         # each token directly reads off the logits for the next token from a lookup table\n",
        "# #         # table dimension: vocab_size x vocab_size\n",
        "# #         print(\"constructor vocab size:\", vocab_size)\n",
        "# #         self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "# #     def forward(self, idx, targets=None):\n",
        "\n",
        "# #         #### 1) get logits\n",
        "# #         # idx and targets are both (B,T) tensor of integers\n",
        "# #         # (B,T,C) batch=4---time=8---channel=vocab size=65\n",
        "# #         # idx: mapping into xb\n",
        "# #         # get prediction scores for every one of the 4x8 positions\n",
        "# #         logits = self.token_embedding_table(idx) # (B,T,C): batch, time, channel\n",
        "\n",
        "\n",
        "# #         #### 2) negative log likelihood\n",
        "# #         # originally, cross entropy func expects (B,C,T) tensor\n",
        "# #         # shape is needed:\n",
        "# #         B, T, C = logits.shape                   # [4, 8, 65]\n",
        "# #         print(\"logits initial:\",logits.shape)\n",
        "\n",
        "# #         # convert logits to one dimension sequence,\n",
        "# #         # then preserve channel dim. (2nd dim)\n",
        "# #         logits = logits.view(B*T, C)             # [32, 65]\n",
        "# #         print(\"logits final:\",logits.shape)\n",
        "\n",
        "# #         # convert targets to one dimension sequence,\n",
        "# #         print(\"targets initial:\",targets.shape)  # [4, 8]\n",
        "# #         targets = targets.view(B*T)\n",
        "# #         print(\"targets final:\",targets.shape)    # [32]\n",
        "\n",
        "# #         loss = F.cross_entropy(logits, targets)  # [32, 65] vs [32]\n",
        "\n",
        "# #         return logits, loss\n",
        "\n",
        "# # m = BigramLanguageModel(vocab_size)\n",
        "# # print(\"xb shape:\", xb.shape)\n",
        "# # print(\"yb shape:\", yb.shape)\n",
        "\n",
        "\n",
        "# # logits, loss = m(xb, yb)\n",
        "\n",
        "# # print()\n",
        "# # print(logits.shape)       # torch.Size([4, 8, 65])\n",
        "# # print(loss)               # we expect -ln(1/65)=4.1...\n"
      ],
      "metadata": {
        "id": "-1kHMP_JKkRa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+Generation module"
      ],
      "metadata": {
        "id": "7umRlo_1dg27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        # table dimension: vocab_size x vocab_size\n",
        "        print(\"constructor vocab size:\", vocab_size)\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        #### 1) get logits\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        # (B,T,C) batch=4---time=8---channel=vocab size=65\n",
        "        # idx: mapping into xb\n",
        "        # get prediction scores for every one of the 4x8 positions\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C): batch, time, channel\n",
        "\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            #### 2) negative log likelihood\n",
        "            # originally, cross entropy func expects (B,C,T) tensor\n",
        "            # shape is needed:\n",
        "            B, T, C = logits.shape                   # [4, 8, 65]\n",
        "\n",
        "            # convert logits to one dimension sequence,\n",
        "            # then preserve channel dim. (2nd dim)\n",
        "            logits = logits.view(B*T, C)             # [32, 65]\n",
        "\n",
        "            # convert targets to one dimension sequence,\n",
        "            targets = targets.view(B*T)              # [4, 8] --> [32]\n",
        "            loss = F.cross_entropy(logits, targets)  # [32, 65] vs [32]\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context for a batch\n",
        "        # takes B,T and extend to B,T+1 ---> T+2 --> ...\n",
        "        #                         B+1,T+1 --> T+2 --> ...\n",
        "\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "\n",
        "            # focus only on the last time step: pred\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distributi  on\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # crates (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print()\n",
        "# print(logits.shape)       # torch.Size([4, 8, 65])\n",
        "print(loss)               # we expect -ln(1/65)=4.1...\n",
        "\n",
        "\n",
        "IDX = torch.zeros((1, 1), dtype=torch.long)  # first char: newline char\n",
        "print(decode(m.generate(idx = IDX,\n",
        "                        max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m4CLeIGTIpj",
        "outputId": "62aa3d13-3e97-4276-c4f6-7a2d1b8c301e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "constructor vocab size: 65\n",
            "\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "-0UfY4jFiZ6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "-JjHnvkhiabc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(1000):\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)   # zero grads for prev step\n",
        "    loss.backward()                         # get grads for all params\n",
        "    optimizer.step()                        # update params\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS8y1zZdikrZ",
        "outputId": "85653adb-db77-4a3d-b463-f4ba8da0ef26"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7218432426452637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long),\n",
        "                        max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8thQXe_jNeO",
        "outputId": "809733e8-4d97-4428-beb6-c5107dc41f3f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "olylvLLko'TMyatyIoconxad.?-tNSqYPsx&bF.oiR;BD$dZBMZv'K f bRSmIKptRPly:AUC&$zLK,qUEy&Ay;ZxjKVhmrdagC-bTop-QJe.H?x\n",
            "JGF&pwst-P sti.hlEsu;w:w a BG:tLhMk,epdhlay'sVzLq--ERwXUzDnq-bn czXxxI&V&Pynnl,s,Ioto!uvixwC-IJXElrgm C-.bcoCPJ\n",
            "IMphsevhO AL!-K:AIkpre,\n",
            "rPHEJUzV;P?uN3b?ohoRiBUENoV3B&jumNL;Aik,\n",
            "xf -IEKROn JSyYWW?n 'ay;:weO'AqVzPyoiBL? seAX3Dot,iy.xyIcf r!!ul-Koi:x pZrAQly'v'a;vEzN\n",
            "BwowKo'MBqF$PPFb\n",
            "CjYX3beT,lZ qdda!wfgmJP\n",
            "DUfNXmnQU mvcv?nlnQF$JUAAywNocd  bGSPyAlprNeQnq-GRSVUP.Ja!IBoDqfI&xJM AXEHV&DKvRS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Math trick in self-attention"
      ],
      "metadata": {
        "id": "kz7g1ozAGkZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### we want tokens to talk each other\n",
        "\n",
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)      # torch.Size([4, 8, 2])\n",
        "print(x.shape)\n",
        "\n",
        "\n",
        "### how to communicate with past:\n",
        "### way-1: average of all preceding elements\n",
        "### ex: 5th token, get info channels from 4th, 3rd, 2nd, 1st step\n",
        "### average them --> summarizes current token, in context of its history\n",
        "\n",
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "\n",
        "print(\"xbow shape:\", xbow.shape)\n",
        "print(\"x shape:\", x.shape)\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "\n",
        "        xprev = x[b,0:t+1] # (t,C)       x[b,:t+1] == x[b,0:t+1]\n",
        "        # print(f\"b:[{b}] t:[{t}] xprev s:\",xprev.shape)  # torch.Size([0...t, 2])\n",
        "\n",
        "        xbow[b,t] = torch.mean(xprev, 0)                # torch.Size([2])\n",
        "        # print(f\"b:[{b}] t:[{t}] xprev s:\",xbow[b,t].shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88prVMB3Ea_s",
        "outputId": "7b7efc6b-29c3-42ae-ef3e-5fa2a028d06b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 2])\n",
            "xbow shape: torch.Size([4, 8, 2])\n",
            "x shape: torch.Size([4, 8, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)     # avr weights for main matrix\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkuaPgOSGzgz",
        "outputId": "23d317c2-68fd-41a3-da5c-aee97cf7534a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfV5fYC7Ugf5",
        "outputId": "060b1e8f-9272-4439-cef3-daf35f12334e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "\n",
        "# (T, T) @ (B, T, C) ----> (B, T, C)\n",
        "# dim mismatch --> PyTorch creates Batch dim for wei:\n",
        "# (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "\n",
        "xbow2 = wei @ x\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYaJI3MHUn2G",
        "outputId": "577993d7-d298-4e53-8fa1-9a2f04f39dc0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "print(tril)\n",
        "print()\n",
        "\n",
        "\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(wei)\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiNLgLRIWPE4",
        "outputId": "7aec1f8d-85e2-493b-c446-efc17bbee747"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "\n",
            "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(wei)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsr7TMq1V_rp",
        "outputId": "be391bcf-a720-45b6-dd04-8415b916d29b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2nd half"
      ],
      "metadata": {
        "id": "UHDTL5HKL9Ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "self attention"
      ],
      "metadata": {
        "id": "axtYb3tYL-tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ver 4: self-attention\n",
        "# small self attention for single head\n",
        "\n",
        "# 4x8 arrangement of tokens\n",
        "# info of each token has 32 dimensions\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)      # torch.Size([4, 8, 2])\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "out = wei @ x\n",
        "\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mhcWYF7L-Ce",
        "outputId": "51d33955-4173-4b48-e51c-0b0f32621bb5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjPYVU_-MODV",
        "outputId": "f0ada26e-2795-40b0-d769-2029c7104209"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O31YPsbAMPGu",
        "outputId": "88b0c359-94f7-42b1-e2e3-fbbf57f56385"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "how to make distribution of this past matrix in a data dependent way"
      ],
      "metadata": {
        "id": "njfUYSm7Ni_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### every single node/token at each pos emit 2 vectors\n",
        "### query: what am i looking for\n",
        "### key: what do i contain\n",
        "\n",
        "### way to get affinites btw tokens:\n",
        "### dot product of keys and queries\n",
        "\n",
        "### my query * all keys of all other tokens\n",
        "### this dot product ==> wei"
      ],
      "metadata": {
        "id": "63V-79xuMPgV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ver 4: self-attention\n",
        "# small self attention for single head\n",
        "\n",
        "# 4x8 arrangement of tokens\n",
        "# info of each token has 32 dimensions\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)      # torch.Size([4, 8, 2])\n",
        "\n",
        "# single head perform self-attention\n",
        "# nn linear is just matrix multiply w/ fixed weights\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias = False)\n",
        "query = nn.Linear(C, head_size, bias = False)\n",
        "value = nn.Linear(C, head_size, bias = False)\n",
        "\n",
        "k = key(x)      # (B,T,16)\n",
        "q = query(x)    # (B,T,16)\n",
        "v = query(x)    # (B,T,16)\n",
        "### all the tokens in positions of B by T arr.\n",
        "### are in parallel and independently produce k, q\n",
        "### no communication yet!\n",
        "\n",
        "\n",
        "\n",
        "wei = q @ k.transpose(-2, -1)     # transpose last two dimensions\n",
        "                                  # (B,T,16) @ (B,16,T) -> (B,T,T)\n",
        "\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "# wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "\n",
        "v = value(x)        # elements we aggregate rather than raw x\n",
        "# out = wei @ x\n",
        "out = wei @ v\n",
        "\n",
        "print(out.shape)\n",
        "\n",
        "\n",
        "\n",
        "### x is like priv info to this token,\n",
        "### 5th token + identity = info is kept in x\n",
        "### here is what i'm interested in (q), here is what I have (k)\n",
        "### if you find me interesting, here is what I will communicate to you (v)\n",
        "\n",
        "### in encoder block, delete \"masked_fill block\":\n",
        "### allow all tokens to communicate each other\n",
        "### in decoder attention block: it has triangular masking\n",
        "### used in autoregressive settings, lang modelling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBehjt7hY8Sp",
        "outputId": "26d9148d-a0ef-4add-cdbe-ea85546e1ab3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-HysVCAdCLB",
        "outputId": "61b7b2e3-5789-49d1-ae83-8c55b04b2a50"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### self attention: keys queries values\n",
        "### are coming from same source=X\n",
        "\n",
        "# # # encoder decoder transformes:\n",
        "# # # queries are from X\n",
        "# # # keys and values from external source from encoder blocks\n",
        "# # # in cross attention, a seperate source of nodes we d like to\n",
        "# # # pull info from into our nodes\n",
        "# # # in self-attention, nodes we d like to look at each other\n",
        "# # # talk to each other\n"
      ],
      "metadata": {
        "id": "b8O2wffBdDRJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sclaed attention: normalization (controls variance)\n",
        "\n",
        "if weights are very positive or very negative\n",
        "softmax converges towards one hot vectors"
      ],
      "metadata": {
        "id": "xbxK3Zapx-RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei_old = q @ k.transpose(-2, -1)  #* head_size**-0.5\n",
        "wei_norm = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "j5uTAlxoyATx"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hd8SyVEyUos",
        "outputId": "d2c05110-6266-4218-edb2-e5007082d213"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1c-_1ahyXO9",
        "outputId": "8bee5765-9b60-4d88-d129-b8af8124ae42"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wei_old.var())\n",
        "print(wei_norm.var())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTEKeb5jyY6G",
        "outputId": "da4b55a1-0a06-4659-ea1d-20390f145445"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(17.4690)\n",
            "tensor(1.0918)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bigram_v4.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJV49mC8_1bB",
        "outputId": "e3f776a9-ec2c-413c-f9a4-fa92740abe25"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.5172, val loss 4.5338\n",
            "step 500: train loss 2.4316, val loss 2.4466\n",
            "step 1000: train loss 2.2167, val loss 2.2462\n",
            "step 1500: train loss 2.0531, val loss 2.1046\n",
            "step 2000: train loss 1.9510, val loss 2.0247\n",
            "step 2500: train loss 1.8656, val loss 1.9584\n",
            "step 3000: train loss 1.8023, val loss 1.9181\n",
            "step 3500: train loss 1.7484, val loss 1.8835\n",
            "step 4000: train loss 1.7072, val loss 1.8522\n",
            "step 4500: train loss 1.6740, val loss 1.8308\n",
            "\n",
            "\n",
            "CLAUNUS:\n",
            "Dood behose, as give, Inew their maid and sor.\n",
            "\n",
            "POLIO:\n",
            "You, swas reanury con:\n",
            "My hensecce glor C azilloke newly.\n",
            "\n",
            "CLAUUS:\n",
            "Tweet of Edwhere folloum, to loour man's with:\n",
            "I warmprince. What way, and disstore\n",
            "If know the vish with three\n",
            "Ih, my reemys reaing to gour are youn,\n",
            "Be in the compod;\n",
            "Of but in that mine that hank or sets of cound.\n",
            "\n",
            "RUmeather hearies! wer, and in Come pardenrom tyner agaunding thou didd?\n",
            "\n",
            "YORK:\n",
            "In twick See, we warm I besseible, not cancher---\n",
            "Whoso think\n",
            "Coumsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bigram_v5.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb_lKzU_AUZU",
        "outputId": "d004fe00-686f-451d-8f3f-832d36055be6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.7321, val loss 4.7310\n",
            "step 500: train loss 2.4659, val loss 2.4818\n",
            "step 1000: train loss 2.3074, val loss 2.3307\n",
            "step 1500: train loss 2.1362, val loss 2.1775\n",
            "step 2000: train loss 2.0047, val loss 2.0698\n",
            "step 2500: train loss 1.9188, val loss 2.0053\n",
            "step 3000: train loss 1.8468, val loss 1.9534\n",
            "step 3500: train loss 1.7950, val loss 1.9127\n",
            "step 4000: train loss 1.7510, val loss 1.8831\n",
            "step 4500: train loss 1.7132, val loss 1.8539\n",
            "\n",
            "DUKE VINCENLEOF\n",
            "DWARD:\n",
            "Now 'ter:\n",
            "LYCULIO:\n",
            "Cliventy sund fer ouse:\n",
            "Pomforing thath-pry delone.\n",
            "\n",
            "FLORIZABELIO:\n",
            "May freas wording You,\n",
            "Thought that and so thell bandablene ody you hlem's bress?\n",
            "\n",
            "Firss, owh had Rome rop stan her Oxchder:\n",
            "Whome lied ever off.\n",
            "ORD LET:\n",
            "Come, Edwited;\n",
            "If thwird some, I see, or trusur now?\n",
            "\n",
            "GLORDY VER:\n",
            ":\n",
            "I mord bord from, did of they edyes tou that and this for tion thou twerentlu.\n",
            "\n",
            "CORIOLANUS:\n",
            "A vay dand stanpert:' nothe, behy swerefor maicin--res\n",
            "TRENVOLIO:\n",
            "Off your c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bigram_v6.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpjYfnvXBs3B",
        "outputId": "5951be26-2302-411f-fc6b-6cab35b7e625"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6660, val loss 4.6666\n",
            "step 500: train loss 2.1907, val loss 2.2330\n",
            "step 1000: train loss 1.8486, val loss 1.9596\n",
            "step 1500: train loss 1.6744, val loss 1.8274\n",
            "step 2000: train loss 1.5765, val loss 1.7482\n",
            "step 2500: train loss 1.5134, val loss 1.7014\n",
            "step 3000: train loss 1.4723, val loss 1.6687\n",
            "step 3500: train loss 1.4354, val loss 1.6349\n",
            "step 4000: train loss 1.4145, val loss 1.6117\n",
            "step 4500: train loss 1.3859, val loss 1.5940\n",
            "\n",
            "GLOUCHESS OF YORK:\n",
            "What I is not is goes must make drownrow have too.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "But be their sweet been good very proft;\n",
            "But, indo Citizen, good lord as from his nature nor\n",
            "The prich that mayfult us. Which will be she,\n",
            "Your challing pames are your bloods.\n",
            "\n",
            "SICK:\n",
            "Althook my son, so do be thinders I love thee deed.\n",
            "\n",
            "GLOUCHERSSET:\n",
            "Thou kstection where thous! with stay seeks of feasts.\n",
            "\n",
            "ESCALUS:\n",
            "Thy jest.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "First Murselve?\n",
            "\n",
            "Third Citizen:\n",
            "Then rator, undeed uport be somment.\n",
            "\n",
            "ISAB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bigram_v2.py"
      ],
      "metadata": {
        "id": "FS-KkQ9_AdHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd0a6af-8f2f-4b70-9752-9d2a30a6010a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6401, val loss 4.6583\n",
            "step 500: train loss 2.4081, val loss 2.4024\n",
            "step 1000: train loss 2.2849, val loss 2.2979\n",
            "step 1500: train loss 2.1879, val loss 2.2196\n",
            "step 2000: train loss 2.1561, val loss 2.1909\n",
            "step 2500: train loss 2.1040, val loss 2.1640\n",
            "step 3000: train loss 2.0892, val loss 2.1669\n",
            "step 3500: train loss 2.0645, val loss 2.1219\n",
            "step 4000: train loss 2.0438, val loss 2.1218\n",
            "step 4500: train loss 2.0138, val loss 2.1080\n",
            "\n",
            "Will beford\n",
            "There,\n",
            "The lay be madisen bube to take rud my dagalanss:\n",
            "Wanthild wick, to bardetless, enaw crome.\n",
            "\n",
            "HERLOYCHESTER:\n",
            "Youns, to tis heart milend;\n",
            "Whines if end not nifatient drove to do;\n",
            "Wil not o.\n",
            "What of liking tear.\n",
            "Mhuse once,\n",
            "Now up Maress, why helvings not\n",
            "To thights;\n",
            "Pithy would thak\n",
            "To Winoun her eiicks to they, bincan enot ear poocto the the the danter, the so;\n",
            "Ang his shat thy fleor their dear?\n",
            "\n",
            "KING ELELONNTIES:\n",
            "With is wards.\n",
            "Wicing's stainin cour and tey Rull stonce thee wi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # batch norm vs layer norm\n",
        "\n",
        "# batch norm: calculate avr&mean, normalize values for all\n",
        "# of the outputs of all of the single neurons\n",
        "# --> across the batch dim., any neuron had gauss dist(output: u:0,std:1)\n",
        "# --> normalizes every single column of input\n",
        "\n",
        "\n",
        "# layer norm: calculate and normalize values per data point\n",
        "# * can deal with seq's, any batch num works, can parallelize\n",
        "# --> normalizes rows\n",
        "\n"
      ],
      "metadata": {
        "id": "2juZ1tLE3Vok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "82m9Sy33-tvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5EpD73OxU6v",
        "outputId": "204907ef-9015-4cb7-d0a8-66cb8c930cbd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}